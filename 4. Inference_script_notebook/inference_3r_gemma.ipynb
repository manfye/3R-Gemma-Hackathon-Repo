{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee36879-d084-442b-a3ce-6c7d47240248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.11: Fast Gemma3N patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.151 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8816042d0d4ebcaeec7ef6f96f3bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "import torch._dynamo.config\n",
    "\n",
    "torch._dynamo.config.recompile_limit = 512\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"qizunlee/gemma3n_E4B_it_ft_3RGarbageClassification\", # or \"unsloth/gemma-3n-E2B-it\"\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    "    # token = \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e85d5a-7ba1-43f6-a9bb-9006aa8668ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"material\": [\n",
      "    {\n",
      "      \"part_name\": \"Coffee cup body\",\n",
      "      \"answer\": \"A: Cardboard\"\n",
      "    },\n",
      "    {\n",
      "      \"part_name\": \"Lid\",\n",
      "      \"answer\": \"E: Plastic\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "PROMPT_FOR_VISION = (\n",
    "    \"You are a garbage classification assistant. Based on the image, identify and classify all distinct parts of the object. \"\n",
    "    \"For each part, determine the type of garbage from the following options: A: Cardboard, B: Glass, C: Metal, D: Paper, E: Plastic, F: Trash. \"\n",
    "    \"Your response must be in a JSON format. The JSON should contain a single key, 'material', which holds an array of objects. \"\n",
    "    \"Each object in the array must have two keys: 'part_name' (a brief description of the item) and 'answer' (the classification from the provided options, in the format 'A: Cardboard'). \"\n",
    "    \"If the image contains multiple distinct parts made of different materials, list each part as a separate object in the 'material' array. \"\n",
    "    \"For example, if the image shows a paper coffee cup with a plastic lid, you should output two separate objects in the array. \"\n",
    "    \"The cup should be classified as 'D: Paper' and the lid as 'E: Plastic'. \"\n",
    "    \"If a part is not classified into a specific category, consider it as 'F: Trash'.\"\n",
    ")\n",
    "\n",
    "image_path = \"WhatsApp Image 2025-08-01 at 23.50.15_38d0037a.jpg\"\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": PROMPT_FOR_VISION}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "# Convert the grayscale image to RGB\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.0, top_p = 0.95, top_k = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed760303-3ac9-482d-8b53-e60c05d95e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
